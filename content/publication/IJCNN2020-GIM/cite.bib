
@article{cossu_continual_2020,
	title = {Continual {Learning} with {Gated} {Incremental} {Memories} for sequential data processing},
	copyright = {All rights reserved},
	url = {http://arxiv.org/abs/2004.04077},
	abstract = {The ability to learn in dynamic, nonstationary environments without forgetting previous knowledge, also known as Continual Learning (CL), is a key enabler for scalable and trustworthy deployments of adaptive solutions. While the importance of continual learning is largely acknowledged in machine vision and reinforcement learning problems, this is mostly under-documented for sequence processing tasks. This work proposes a Recurrent Neural Network (RNN) model for CL that is able to deal with concept drift in input distribution without forgetting previously acquired knowledge. We also implement and test a popular CL approach, Elastic Weight Consolidation (EWC), on top of two different types of RNNs. Finally, we compare the performances of our enhanced architecture against EWC and RNNs on a set of standard CL benchmarks, adapted to the sequential data processing scenario. Results show the superior performance of our architecture and highlight the need for special solutions designed to address CL in RNNs.},
	urldate = {2020-04-09},
	journal = {Proceedings of the 2020 International Joint Conference on Neural Networks (IJCNN 2020},
	author = {Cossu, Andrea and Carta, Antonio and Bacciu, Davide},
	month = apr,
	year = {2020},
	note = {arXiv: 2004.04077},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning}
}