---
# Documentation: https://sourcethemes.com/academic/docs/managing-content/

title: 'Ex-Model: Continual Learning from a Stream of Trained Models'
subtitle: ''
summary: ''
authors:
- Antonio Carta
- Andrea Cossu
- Vincenzo Lomonaco
- Davide Bacciu
tags:
- '"Computer Science - Artificial Intelligence"'
- '"Computer Science - Computer Vision and Pattern Recognition"'
- '"Computer Science - Machine Learning"'
categories: []
date: '2021-01-01'
lastmod: 2021-12-14T16:41:53+01:00
featured: false
draft: false

# Featured image
# To use, add an image named `featured.jpg/png` to your page's folder.
# Focal points: Smart, Center, TopLeft, Top, TopRight, Left, Right, BottomLeft, Bottom, BottomRight.
image:
  caption: ''
  focal_point: ''
  preview_only: false

# Projects (optional).
#   Associate this post with one or more of your projects.
#   Simply enter your project's folder or file name without extension.
#   E.g. `projects = ["internal-project"]` references `content/project/deep-learning/index.md`.
#   Otherwise, set `projects = []`.
projects: []
publishDate: '2021-12-14T15:41:53.342681Z'
publication_types:
- '2'
abstract: Learning continually from non-stationary data streams is a challenging research
  topic of growing popularity in the last few years. Being able to learn, adapt, and
  generalize continually in an efficient, effective, and scalable way is fundamental
  for a sustainable development of Artificial Intelligent systems. However, an agent-centric
  view of continual learning requires learning directly from raw data, which limits
  the interaction between independent agents, the efficiency, and the privacy of current
  approaches. Instead, we argue that continual learning systems should exploit the
  availability of compressed information in the form of trained models. In this paper,
  we introduce and formalize a new paradigm named \"Ex-Model Continual Learning\"
  (ExML), where an agent learns from a sequence of previously trained models instead
  of raw data. We further contribute with three ex-model continual learning algorithms
  and an empirical setting comprising three datasets (MNIST, CIFAR-10 and CORe50),
  and eight scenarios, where the proposed algorithms are extensively tested. Finally,
  we highlight the peculiarities of the ex-model paradigm and we point out interesting
  future research directions.
publication: '*arXiv*'
url_pdf: http://arxiv.org/abs/2112.06511
---
