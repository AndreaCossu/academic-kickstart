---
# Documentation: https://sourcethemes.com/academic/docs/managing-content/

title: Continual Learning with Echo State Networks
subtitle: ''
summary: ''
authors:
- Andrea Cossu
- Davide Bacciu
- Antonio Carta
- Claudio Gallicchio
- Vincenzo Lomonaco
tags:
- '"Computer Science - Artificial Intelligence"'
- '"Computer Science - Machine Learning"'
categories: []
date: '2021-01-01'
lastmod: 2021-06-29T12:39:17+02:00
featured: false
draft: false

# Featured image
# To use, add an image named `featured.jpg/png` to your page's folder.
# Focal points: Smart, Center, TopLeft, Top, TopRight, Left, Right, BottomLeft, Bottom, BottomRight.
image:
  caption: ''
  focal_point: ''
  preview_only: false

# Projects (optional).
#   Associate this post with one or more of your projects.
#   Simply enter your project's folder or file name without extension.
#   E.g. `projects = ["internal-project"]` references `content/project/deep-learning/index.md`.
#   Otherwise, set `projects = []`.
projects: []
publishDate: '2021-06-29T10:39:17.519985Z'
publication_types:
- '1'
abstract: Continual Learning (CL) refers to a learning setup where data is non stationary
  and the model has to learn without forgetting existing knowledge. The study of CL
  for sequential patterns revolves around trained recurrent networks. In this work,
  instead, we introduce CL in the context of Echo State Networks (ESNs), where the
  recurrent component is kept fixed. We provide the first evaluation of catastrophic
  forgetting in ESNs and we highlight the benefits in using CL strategies which are
  not applicable to trained recurrent models. Our results confirm the ESN as a promising
  model for CL and open to its use in streaming scenarios.
publication: '*ESANN*'
url_pdf: https://doi.org/10.1016/j.neunet.2021.07.021
url_code: https://github.com/Pervasive-AI-Lab/ContinualLearning-EchoStateNetworks
url_dataset:
url_poster:
url_project:
url_slides:
url_source:
url_video:
---
