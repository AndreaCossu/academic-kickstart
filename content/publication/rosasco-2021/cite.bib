@inproceedings{rosasco2021,
 abstract = {Replay strategies are Continual Learning techniques which mitigate catastrophic forgetting by keeping a buffer of patterns from previous experiences, which are interleaved with new data during training. The amount of patterns stored in the buffer is a critical parameter which largely influences the final performance and the memory footprint of the approach. This work introduces Distilled Replay, a novel replay strategy for Continual Learning which is able to mitigate forgetting by keeping a very small buffer (1 pattern per class) of highly informative samples. Distilled Replay builds the buffer through a distillation process which compresses a large dataset into a tiny set of informative examples. We show the effectiveness of our Distilled Replay against popular replay-based strategies on four Continual Learning benchmarks.},
 archiveprefix = {arXiv},
 author = {Rosasco, Andrea and Carta, Antonio and Cossu, Andrea and Lomonaco, Vincenzo and Bacciu, Davide},
 booktitle = {1st International Workshop on Continual Semi-Supervised Learning (CSSL) at IJCAI},
 copyright = {All rights reserved},
 eprint = {2103.15851},
 eprinttype = {arxiv},
 file = {/home/andrea/Zotero/storage/TQAZ3HXC/Rosasco et al. - 2021 - Distilled Replay Overcoming Forgetting through Sy.pdf;/home/andrea/Zotero/storage/QV4SK36M/2103.html},
 keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
 shorttitle = {Distilled Replay},
 title = {Distilled Replay: Overcoming Forgetting through Synthetic Samples},
 url = {http://arxiv.org/abs/2103.15851},
 urldate = {2021-08-01},
 year = {2021}
}

